{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "836cb59a",
   "metadata": {},
   "source": [
    "# üìä Scraped Database Processing & Salary Intelligence\n",
    "\n",
    "## Project Introduction\n",
    "This Jupyter Notebook focuses on the processing, cleansing, and enrichment of data extracted from various job boards (Manfred and Tecnoempleo). The primary objective is to decode the current state of the Spanish IT labor market by answering key strategic questions:\n",
    "* Which roles command the highest compensation in the current market?\n",
    "* What is the real impact of seniority on salary scaling?\n",
    "* How has the \"100% On-site\" model shifted towards Remote and Hybrid work?\n",
    "\n",
    "## Methodology Overview\n",
    "Due to the prevailing **salary opacity** in the sector, this project does not merely aggregate data. It implements a **Data Engineering pipeline** that:\n",
    "1. **Normalizes** disparate data structures from multiple web sources.\n",
    "2. **Triangulates** missing information using industry-standard benchmarks ([Randstad Research 2026](https://www.randstadresearch.es/tendencias-salariales/?gad_campaignid=23330767618), `Sector de IT & Telecom`) .\n",
    "3. **Consolidates** a high-integrity dataset ready for Business Intelligence exploitation.\n",
    "\n",
    "By bridging the gap between raw web-scraped text and structured insights, this analysis provides a transparent view of the IT compensation landscape in 2026.\n",
    "\n",
    "## üõ†Ô∏è Phase 0: Setup and Environment Configuration\n",
    "\n",
    "This initial section establishes the technical foundation of the project. We import the fundamental libraries required for the entire data lifecycle:\n",
    "* **Ingestion:** `Playwright` for browser automation and dynamic web data extraction.\n",
    "* **Processing:** `Pandas` and `NumPy` for data structure manipulation and wrangling.\n",
    "* **Visualization:** `Seaborn` and `Matplotlib` for preliminary statistical analysis.\n",
    "\n",
    "The goal is to ensure a scalable and efficient environment for processing the collected job postings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3250464",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import sys\n",
    "\n",
    "# Importar patrones de regex\n",
    "sys.path.insert(0, './src')\n",
    "from src.regex_salarios import REGEX_SALARIOS"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d93645b4",
   "metadata": {},
   "source": [
    "## üß¨ Phase 1: Manfred Data Processing\n",
    "\n",
    "Manfred serves as our primary source of **real-world data**. It is characterized by high transparency, allowing us to capture explicit salary ranges in the majority of its postings.\n",
    "\n",
    "**Architectural Decision:**\n",
    "In this project, we opted to generate separate databases for each source, as different websites present information in distinct structures. Consequently, the data processing logic is isolated within this stage rather than during the scraping phase. Manfred provides a concise summary containing all key data points, requiring minimal processing and significantly simplifying data extraction from Natural Language text.\n",
    "\n",
    "**Actions taken:**\n",
    "1. Text cleansing and normalization of labels (Seniority levels and Job Roles).\n",
    "2. Conversion of salary ranges into mean values for statistical analysis.\n",
    "3. Initial Exploratory Data Analysis (EDA) to validate sample distribution.\n",
    "\n",
    "This database acts as an \"anchor\" to understand actual market salaries before proceeding with estimations for other sources."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa12faf0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Cargamos los datos brutos que conseguiste\n",
    "df = pd.read_csv(\"../data/ofertas_generico.csv\")\n",
    "\n",
    "# 2. Funci√≥n \"limpiadora\" (Parsing)\n",
    "def limpiar_oferta(row):\n",
    "    texto = row['contenido']\n",
    "    \n",
    "    # Regex para sueldo bruto mensual en euros (puede tener formato ‚Ç¨2,838K o ‚Ç¨2838 o ‚Ç¨2.838)\n",
    "    sueldo_pattern = r'‚Ç¨\\s*([0-9,.K-]+?)(?:\\s*\\||$)'\n",
    "    sueldo_match = re.search(sueldo_pattern, texto)\n",
    "    sueldo = 0\n",
    "    if sueldo_match:\n",
    "        cantidad_str = sueldo_match.group(1).strip()\n",
    "        # Extraer el valor medio (ej: \"30-50K\" -> \"40\")\n",
    "        if '-' in cantidad_str:\n",
    "            partes = cantidad_str.split('-')\n",
    "            if partes[1].endswith('K'):\n",
    "                partes[1] = partes[1].replace('K','')\n",
    "                partes[0] = int(partes[0]) * 1000\n",
    "                partes[1] = int(partes[1]) * 1000\n",
    "            cantidad_str = str((int(partes[0]) + int(partes[1])) // 2)\n",
    "        # Convertir K a miles\n",
    "        if 'K' in cantidad_str:\n",
    "            cantidad_str = cantidad_str.replace('K', '').replace(',', '').replace('.', '')\n",
    "            cantidad = str(int(float(cantidad_str)) * 1000)\n",
    "        else:\n",
    "            cantidad = cantidad_str.replace(',', '').replace('.', '')\n",
    "        sueldo = int(cantidad)\n",
    "    \n",
    "    # Ubicaci√≥n\n",
    "    ubicacion_pattern = r'UBICACI√ìN\\s*\\|\\s*([^\\|]+)'\n",
    "    ubicacion_match = re.search(ubicacion_pattern, texto)\n",
    "    ubicacion = ubicacion_match.group(1).strip() if ubicacion_match else \"N/A\"\n",
    "\n",
    "    # Modalidad de trabajo - m√°s robusta\n",
    "    modalidad = \"N/A\"\n",
    "    # Primero buscamos el patr√≥n \"TELETRABAJO | porcentaje\"\n",
    "    modalidad_pattern = r'TELETRABAJO\\s*\\|\\s*(\\d+)%'\n",
    "    modalidad_match = re.search(modalidad_pattern, texto)\n",
    "    if modalidad_match:\n",
    "        porcentaje = int(modalidad_match.group(1))\n",
    "        if porcentaje == 100:\n",
    "            modalidad = \"Telem√°tico\"\n",
    "        elif porcentaje == 0:\n",
    "            modalidad = \"Presencial\"\n",
    "        else:\n",
    "            modalidad = \"H√≠brido\"\n",
    "    else:\n",
    "        # Si no encontramos TELETRABAJO, buscamos palabras clave\n",
    "        if re.search(r'H√≠brido', texto, re.IGNORECASE):\n",
    "            modalidad = \"H√≠brido\"\n",
    "        elif re.search(r'Presencial', texto, re.IGNORECASE):\n",
    "            modalidad = \"Presencial\"\n",
    "        elif re.search(r'Telem√°tico|Remoto|100% remoto', texto, re.IGNORECASE):\n",
    "            modalidad = \"Telem√°tico\"\n",
    "    \n",
    "    # Generamos un array, por el pipe \"|\" que pusimos en el scraper\n",
    "    partes = [p.strip() for p in texto.split('|')]\n",
    "    \n",
    "    return pd.Series({\n",
    "        'Puesto': partes[2] if len(partes) > 0 else \"N/A\",\n",
    "        # 'Empresa': partes[1] if len(partes) > 1 else \"N/A\",\n",
    "        'Sueldo_Bruto': sueldo if sueldo != 0 else \"N/A\",\n",
    "        'Ubicaci√≥n': ubicacion,\n",
    "        'Modalidad_Trabajo': modalidad,\n",
    "        'URL': row['url'],\n",
    "        'Fecha_Captura': row['fecha_captura']\n",
    "    })\n",
    "\n",
    "def categorizar_rol(texto):\n",
    "    texto = str(texto).lower()\n",
    "    if 'analyst' in texto or 'analista' in texto: return 'Data Analyst'\n",
    "    if 'engineer' in texto or 'ingeniero' in texto: return 'Data Engineer'\n",
    "    if 'science' in texto or 'cient√≠fico' in texto: return 'Data Scientist'\n",
    "    if 'backend' in texto: return 'Backend/Fullstack Developer'\n",
    "    if 'fullstack' in texto: return 'Backend/Fullstack Developer'\n",
    "    return 'IT Generalist'\n",
    "\n",
    "def categorizar_nivel(texto):\n",
    "    texto = str(texto).lower()\n",
    "    if any(w in texto for w in ['senior', 'sr', 'lead', 'principal', 'cio']): return 'Senior'\n",
    "    if any(w in texto for w in ['junior', 'jr', 'entry', 'trainee']): return 'Junior'\n",
    "    return 'Mid'\n",
    "\n",
    "# 3. Aplicamos la limpieza a todo el DataFrame\n",
    "df_manfred = df.apply(limpiar_oferta, axis=1)\n",
    "# Si es Telem√°tico, rellenamos la ubicaci√≥n con 'Remoto' para que el gr√°fico sea m√°s limpio\n",
    "df_manfred.loc[df_manfred['Modalidad_Trabajo'] == 'Telem√°tico', 'Ubicaci√≥n'] = 'Remoto'\n",
    "df_manfred['Rol'] = df_manfred['Puesto'].apply(categorizar_rol)\n",
    "df_manfred['Nivel'] = df_manfred['Puesto'].apply(categorizar_nivel)\n",
    "df_manfred['Fuente'] = 'Manfred'\n",
    "\n",
    "# 4. Ver el resultado (esto en VS Code se ve como una tabla de Excel preciosa)\n",
    "df_manfred.info()\n",
    "df_manfred = df_manfred.sort_values('Sueldo_Bruto', ascending=True)\n",
    "df_manfred.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2696d44",
   "metadata": {},
   "source": [
    "### ‚ö†Ô∏è Sample Limitations and Data Integrity\n",
    "The processed Manfred dataset is now clean, with no null or empty values. However, we face a critical challenge: the sample consists of only **33 job postings**. \n",
    "\n",
    "Any conclusions drawn at this stage lack sufficient statistical power to be considered definitive. Furthermore, most classes are **unbalanced**, which could introduce significant bias. To draw robust, data-driven conclusions, we must expand our dataset by integrating further sources."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6271895",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuramos el estilo\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.set_theme(style=\"whitegrid\")\n",
    "\n",
    "# Ordenamos por sueldo para que el gr√°fico sea legible\n",
    "df_sorted = df_manfred.sort_values('Sueldo_Bruto', ascending=False)\n",
    "\n",
    "# Creamos el gr√°fico\n",
    "ax = sns.barplot(x='Sueldo_Bruto', y='Puesto', data=df_sorted, palette='viridis')\n",
    "\n",
    "# A√±adimos etiquetas de datos\n",
    "plt.title('Top Ofertas por Salario Bruto (Manfred)', fontsize=15)\n",
    "plt.xlabel('Salario Bruto Anual (‚Ç¨)', fontsize=12)\n",
    "plt.ylabel('Posici√≥n', fontsize=12)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "648c67a3",
   "metadata": {},
   "source": [
    "### üìä Work Modality vs. Salary\n",
    "In an attempt to extract early insights, we calculate the correlation between work modality and salary. The core question: *Do on-site roles offer higher compensation in the current market?*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2c2c18c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Agrupamos por modalidad y sacamos estad√≠sticas b√°sicas\n",
    "stats_modalidad = df_manfred.groupby('Modalidad_Trabajo')['Sueldo_Bruto'].agg(['mean', 'median', 'count', 'min', 'max'])\n",
    "print(\"Estad√≠sticas de Sueldo por Modalidad:\")\n",
    "print(stats_modalidad)\n",
    "\n",
    "# 2. Visualizaci√≥n: Boxplot (Distribuci√≥n de sueldos)\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.boxplot(x='Modalidad_Trabajo', y='Sueldo_Bruto', data=df_manfred, palette=\"Set2\")\n",
    "plt.title('Distribuci√≥n de Sueldos por Modalidad de Trabajo', fontsize=15)\n",
    "plt.ylabel('Sueldo Bruto Anual (‚Ç¨)')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ef4b3fb",
   "metadata": {},
   "source": [
    "#### Preliminary Findings:\n",
    "Given the sample size (23 Hybrid vs. 9 Remote/Teleworking), the classes are unbalanced, and any immediate conclusions may be skewed. However, based on this specific sample of 33 postings, **Remote roles tend to command higher salaries** than their Hybrid counterparts."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd731c59",
   "metadata": {},
   "source": [
    "## üìà Phase 2: Tecnoempleo Processing and Market Benchmarking\n",
    "\n",
    "Tecnoempleo provides the largest volume of job postings but presents a critical challenge: **salary opacity**. To address this data gap without biasing the analysis, we apply a **data triangulation** technique.\n",
    "\n",
    "---\n",
    "<p align=\"center\">\n",
    "  <img src=\"../03_Enrichment/Captura%20Benchmark%20Randstad.png\" alt=\"Randstad Salary Trends 2026\">\n",
    "  <br>\n",
    "  <sub><b>Figure 1:</b> Reference salary ranges for the IT sector in Spain (Source: Randstad Research 2026).</sub>\n",
    "</p>\n",
    "\n",
    "---\n",
    "\n",
    "**Benchmarking Methodology:**\n",
    "To impute missing salaries, we use the **2026 Salary Trends Report from Randstad Research** as a reference. This report provides average salary ranges segmented by profile and experience within the Spanish IT & Telecom sector.\n",
    "\n",
    "**Market Evidence:**\n",
    "As shown in the Randstad data, profiles such as **Data Analysts (‚Ç¨23k - ‚Ç¨57k)** or **Developers (‚Ç¨22k - ‚Ç¨81k)** exhibit massive seniority gaps. This external evidence validates our assignment logic and allows us to complete the dataset in a manner consistent with the current socio-economic reality."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61be395d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Cargar el dataset de tecnoempleo\n",
    "df_tecno = pd.read_csv(\"../data/ofertas_tecnoempleo_raw.csv\")\n",
    "\n",
    "# Convertir a min√∫sculas una vez\n",
    "df_tecno['contenido'] = df_tecno['contenido'].apply(str.lower)\n",
    "\n",
    "# Lista de ciudades espa√±olas principales\n",
    "CIUDADES_ESPA√ëA = {\n",
    "  'madrid', 'barcelona', 'valencia', 'sevilla', 'bilbao', 'm√°laga', 'murcia',\n",
    "  'palma', 'las palmas', 'c√≥rdoba', 'valladolid', 'vigo', 'gij√≥n', 'eibar',\n",
    "  'albacete', 'alicante', 'almer√≠a', '√°vila', 'badajoz', 'burgos', 'c√°diz',\n",
    "  'castell√≥n', 'ciudad real', 'cuenca', 'girona', 'guadalajara', 'huelva',\n",
    "  'huesca', 'ja√©n', 'le√≥n', 'l√©rida', 'logro√±o', 'lugo', 'navarra', 'orense',\n",
    "  'oviedo', 'palencia', 'pamplona', 'pontevedra', 'salamanca', 'santander',\n",
    "  'segovia', 'soria', 'tarragona', 'teruel', 'toledo', 'tortosa', 'zamora',\n",
    "  'zaragoza', 'tenerife', 'ibiza', 'mallorca', 'menorca', 'ceuta', 'melilla',\n",
    "  'alcobendas', 'legan√©s', 'getafe', 'm√≥stoles', 'torrej√≥n', 'alcal√°'\n",
    "}\n",
    "\n",
    "# --- MENTALIDAD SPARK: Definimos funciones puras de transformaci√≥n ---\n",
    "def extract_salary(text):\n",
    "  if pd.isna(text): \n",
    "    return None\n",
    "  salaries = []\n",
    "  for pattern_dict in REGEX_SALARIOS:\n",
    "    patron = pattern_dict['patron']\n",
    "    matches = re.findall(patron, text, re.IGNORECASE)\n",
    "    for m in matches:\n",
    "      # m puede ser string o tuple (si hay grupos de captura)\n",
    "      if isinstance(m, tuple):\n",
    "        m = m[0] if m[0] else m[1] if len(m) > 1 else None\n",
    "      \n",
    "      if m:\n",
    "        # Normalizar\n",
    "        num_str = str(m).replace('.', '').replace(',', '').replace('K', '').replace('k', '')\n",
    "        if num_str.isdigit():\n",
    "          val = int(num_str)\n",
    "          if val < 10000:  # Si < 10k, probablemente est√° en K\n",
    "            val = val * 1000\n",
    "          if 10000 <= val <= 500000:\n",
    "            salaries.append(val)\n",
    "    \n",
    "  return max(salaries) if salaries else None\n",
    "\n",
    "def extract_location(text):\n",
    "  # SOLO buscar patrones muy espec√≠ficos y fiables\n",
    "  location_patterns = [\n",
    "    r'(?:ubicaci√≥n|localizaci√≥n|location)[:\\s]+([a-z√°√©√≠√≥√∫√±\\s]+?)(?:,|\\.|\\||$)',  # ubicaci√≥n: Madrid\n",
    "    r'\\(([a-z√°√©√≠√≥√∫√±\\s]+)\\)',  # (Madrid)\n",
    "    r'(?:en|based in|based at)\\s+([a-z√°√©√≠√≥√∫√±]+)(?:\\s|,|\\.)',  # en Madrid o en Barcelona\n",
    "  ]\n",
    "  \n",
    "  for pattern in location_patterns:\n",
    "    match = re.search(pattern, text, re.IGNORECASE)\n",
    "    if match:\n",
    "      loc = match.group(1).strip()\n",
    "      loc_lower = loc.lower()\n",
    "      # Solo retorna si est√° en lista de ciudades conocidas\n",
    "      if loc_lower in CIUDADES_ESPA√ëA:\n",
    "        return loc.capitalize()\n",
    "  \n",
    "  return \"No especificada\"\n",
    "\n",
    "def extract_modality(text):\n",
    "  if any(word in text for word in ['remoto', 'telem√°tico', 'teletrabajo', 'home office', '100% remoto', 'remote']):\n",
    "    return 'Remoto'\n",
    "  if any(word in text for word in ['h√≠brido', 'presencial', 'oficina', 'on-site']):\n",
    "    return 'H√≠brido/Presencial'\n",
    "  return 'No especificado'\n",
    "\n",
    "\n",
    "# Usamos .apply() en lugar de list comprehensions con zip para evitar las tuplas\n",
    "df_tecno['Sueldo_Bruto'] = df_tecno['contenido'].apply(extract_salary)\n",
    "df_tecno['Ubicaci√≥n'] = df_tecno['contenido'].apply(extract_location)\n",
    "df_tecno['Modalidad_Trabajo'] = df_tecno['contenido'].apply(extract_modality)\n",
    "\n",
    "# Estas ya las ten√≠as bien enfocadas\n",
    "df_tecno['Rol'] = df_tecno['Puesto'].apply(categorizar_rol)\n",
    "df_tecno['Nivel'] = (df_tecno['Puesto'] + \" \" + df_tecno['contenido']).apply(categorizar_nivel)\n",
    "\n",
    "df_tecno['Sueldo_Bruto'] = pd.to_numeric(df_tecno['Sueldo_Bruto'], errors='coerce')\n",
    "df_tecno_ready = df_tecno[['Puesto', 'Sueldo_Bruto', 'Ubicaci√≥n', 'Modalidad_Trabajo', 'Rol', 'Nivel', 'Fuente', 'URL', 'Fecha_Captura']]\n",
    "\n",
    "df_tecno_ready.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "067f6ed5",
   "metadata": {},
   "source": [
    "## üîó Phase 3: Data Integration and Final Consolidation\n",
    "\n",
    "In this final stage, we merge both sources (`Manfred` + `Tecnoempleo`) into a single master dataset structured for Power BI.\n",
    "\n",
    "**Data Integrity Criteria:**\n",
    "We have implemented a logic focused on the **prevalence of real data**. In any record where an explicit salary was provided by the original source, it takes absolute priority over the benchmark estimation. This ensures the final analysis remains as faithful as possible to the original job postings, using estimation only as a tool to prevent loss of representativeness in the total volume.\n",
    "\n",
    "The result is a master dataset of **119 records**, fully optimized for visual and strategic exploitation."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1139047e",
   "metadata": {},
   "source": [
    "### üõ†Ô∏è Creating the Hays Reference Benchmark\n",
    "Unlike job boards, Hays bases its data on actual placements rather than just active ads. To account for this, we‚Äôve built a secondary reference dataframe that maps roles and seniority levels across Spain‚Äôs main tech hubs using the Hays 2026 guide. This second data source adds an extra layer of validation, making our salary benchmark much more reliable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "586fcc22",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Seleccionamos √∫nicamente las columnas necesarias\n",
    "df_manfred = df_manfred[['Puesto', 'Sueldo_Bruto', 'Ubicaci√≥n', 'Modalidad_Trabajo', 'Rol', 'Nivel', 'Fuente', 'URL', 'Fecha_Captura']]\n",
    "df_total = pd.concat([df_manfred, df_tecno_ready], ignore_index=True)\n",
    "\n",
    "df_benchmark_randstad = pd.read_csv(\"../03_Enrichment/benchmark_sueldo_medio_Randstad.csv\")\n",
    "df_benchmark_hays = pd.read_csv(\"../03_Enrichment/benchmark_sueldo_medio_Hays.csv\")\n",
    "\n",
    "df_final = pd.merge(\n",
    "    df_total, \n",
    "    df_benchmark_randstad[['Rol', 'Nivel', 'Sueldo_Aprox_Mercado']], \n",
    "    on=['Rol', 'Nivel'], \n",
    "    how='left'\n",
    ").rename(columns={'Sueldo_Aprox_Mercado': 'Sueldo_Randstad'})\n",
    "\n",
    "df_final = pd.merge(\n",
    "    df_final, \n",
    "    df_benchmark_hays[['Rol', 'Nivel', 'Sueldo_Aprox_Mercado']], \n",
    "    on=['Rol', 'Nivel'],\n",
    "    how='left'\n",
    ").rename(columns={'Sueldo_Aprox_Mercado': 'Sueldo_Hays'})\n",
    "\n",
    "df_final['Sueldo_Aprox_Mercado'] = df_final[['Sueldo_Randstad', 'Sueldo_Hays']].mean(axis=1)\n",
    "\n",
    "# Limpieza: Borramos las columnas temporales y la 'Provincia' sobrante del merge\n",
    "df_final.drop(columns=['Sueldo_Randstad', 'Sueldo_Hays'], inplace=True)\n",
    "df_final['Sueldo_Final'] = df_final['Sueldo_Bruto'].fillna(df_final['Sueldo_Aprox_Mercado'])\n",
    "df_final['Tipo_Dato'] = np.where(df_final['Sueldo_Bruto'].notna(), 'Real', 'Estimado')\n",
    "\n",
    "df_final.to_csv(\"../data/dataset_final.csv\", index=False, encoding='utf-8-sig')\n",
    "\n",
    "print(f\"‚úÖ Proceso terminado. Tenemos {len(df_final)} ofertas listas.\")\n",
    "print(f\"üìä Reales: {len(df_final[df_final['Tipo_Dato'] == 'Real'])} | Estimadas: {len(df_final[df_final['Tipo_Dato'] == 'Estimado'])}\")\n",
    "\n",
    "df_final[df_final['Fuente'] == 'Tecnoempleo'].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b318e40b",
   "metadata": {},
   "source": [
    "## üí° Final Strategic Insight: The Seniority Premium\n",
    "\n",
    "The analysis identifies a **\"Seniority Premium\" of 119,5%**, confirming that the market value of a Senior profile is double that of a Junior. \n",
    "\n",
    "This extreme salary gap, combined with the near-absence of entry-level vacancies (**n=1 out of 119 total offers**), provides clear evidence of a **high-barrier market**. Companies are willing to assume a 100% cost premium to acquire immediate autonomy rather than investing in the development of emerging talent.\n",
    "\n",
    "---\n",
    "> **Methodological Note: Data Ensemble & Triangulation**\n",
    "> \n",
    "> To solve the lack of salary transparency, we moved from a single-source estimation to a **Multi-source Ensemble Model**. \n",
    "> \n",
    "> By averaging **Randstad Research** (comprehensive national data) and **Hays** (specialized hub data), we achieve:\n",
    "> 1. **Bias Reduction:** We don't rely on the criteria of a single consultant, verifying the data we obtained.\n",
    "> 2. **Geographic Precision:** We adjust the estimation in high-cost cities where Hays provides deeper granularity.\n",
    "> 3. **Statistical Robustness:** The \"Ground Truth\" used for our final Seniority Gap analysis (119,5%) is now backed by two of the most reputable sources in the Spanish labor market.\n",
    ">"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "proyecto-bi-manfred",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
